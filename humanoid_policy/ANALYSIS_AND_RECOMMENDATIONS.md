# Training Analysis & Recommendations

## Run Comparison

### Run 1 (2025-12-30_01-31-53) - 1000 Episoden, 1024 envs:
- Episode 100: **1662.9**
- Episode 500: **1684.6** (Peak)
- Episode 1000: **1637.0** (stabil)

### Run 2 (2025-12-30_16-23-21) - 5000 Episoden, 2048 envs:
- Episode 100: **1671.8** (besser Start)
- Episode 1400: **1680.9** (Peak)
- Episode 5000: **1332.7** (stark abgefallen - Overfitting!)

## Problem: Overfitting in Run 2

**Symptom:** Reward fÃ¤llt nach Peak stark ab (1680 â†’ 1332)
**Ursache:** Zu viele Episoden ohne Early Stopping / Learning Rate Decay

---

## âœ… UMSETZUNGS-LISTE: Ã„nderungen fÃ¼r Balance-Fokus

### 1. **PPO Config (rl_games_ppo_cfg.yaml) - Anymal Settings Ã¼bernehmen**

**NN Layers (Anymal Standard):**
- âœ… `units: [256, 128, 64]` â†’ `[512, 256, 128]` (grÃ¶ÃŸeres Netz)

**Hyperparameter (Anymal Standard):**
- âœ… `horizon_length: 64` â†’ `24` (Anymal: 24, stabiler)
- âœ… `minibatch_size: 32768` â†’ `16384` (Anymal: 16384, stabiler)
- âœ… `learning_rate: 3e-4` â†’ **BLEIBT** (Anymal: 3e-4)
- âœ… `e_clip: 0.2` â†’ **BLEIBT** (Anymal: 0.2)
- âœ… `mini_epochs: 5` â†’ **BLEIBT** (Anymal: 5)
- âœ… `max_epochs: 5000` â†’ `2000` (verhindert Overfitting)
- âœ… `save_best_after: 100` â†’ `50` (frÃ¼heres Best-Model Saving)

**Datei:** `humanoid_policy/source/humanoid_policy/humanoid_policy/tasks/direct/humanoid_policy/agents/rl_games_ppo_cfg.yaml`

---

### 2. **Rewards - NUR Balance, KEINE Bewegung**

**Velocity Reward entfernen (bereits deaktiviert):**
- âœ… `rew_scale_forward_vel = 0.0` â†’ **BLEIBT** (bereits deaktiviert)

**Penalties reduzieren (weniger aggressiv fÃ¼r Balance):**
- âœ… `rew_scale_joint_vel: -0.001` â†’ `-0.0005` (weniger aggressiv)
- âœ… `rew_scale_base_vel: -0.01` â†’ `-0.005` (erlaubt kleine Balance-Bewegungen)

**Positive Rewards (bleiben):**
- âœ… `rew_scale_alive = 2.0` â†’ **BLEIBT**
- âœ… `rew_scale_upright = 1.0` â†’ **BLEIBT**
- âœ… `rew_scale_foot_contact = 0.5` â†’ **BLEIBT**

**Negative Rewards (bleiben):**
- âœ… `rew_scale_terminated = -5.0` â†’ **BLEIBT**
- âœ… `rew_scale_action = -0.0001` â†’ **BLEIBT**
- âœ… `rew_scale_action_rate = -0.001` â†’ **BLEIBT**
- âœ… `rew_scale_base_ang_vel = 0.0` â†’ **BLEIBT**
- âœ… `rew_scale_joint_limit = -0.1` â†’ **BLEIBT**

**Datei:** `humanoid_policy/source/humanoid_policy/humanoid_policy/tasks/direct/humanoid_policy/humanoid_policy_env_cfg.py`

---

### 3. **Episode Length - Schnelleres Lernen**

- âœ… `episode_length_s: 10.0` â†’ `5.0` (schnelleres Lernen, mehr Resets)

**Datei:** `humanoid_policy/source/humanoid_policy/humanoid_policy/tasks/direct/humanoid_policy/humanoid_policy_env_cfg.py`

---

### 4. **Training Strategy**

**Environment Anzahl:**
- âœ… Training mit `--num_envs=1024` (Run 1 war stabiler als Run 2)

**Episoden:**
- âœ… Training mit `max_epochs=2000` (verhindert Overfitting)

---

## ðŸ“‹ Zusammenfassung der Ã„nderungen

### PPO Config (YAML):
1. `units: [512, 256, 128]` (von [256, 128, 64])
2. `horizon_length: 24` (von 64)
3. `minibatch_size: 16384` (von 32768)
4. `max_epochs: 2000` (von 5000)
5. `save_best_after: 50` (von 100)

### Env Config (Python):
1. `rew_scale_joint_vel: -0.0005` (von -0.001)
2. `rew_scale_base_vel: -0.005` (von -0.01)
3. `episode_length_s: 5.0` (von 10.0)
4. `rew_scale_forward_vel: 0.0` (bleibt - bereits deaktiviert)

---

## ðŸŽ¯ Ziel: Reine Balance-Policy

**Fokus:**
- âœ… Roboter soll **NUR** aufrecht stehen bleiben
- âœ… **KEINE** VorwÃ¤rtsbewegung
- âœ… **KEINE** seitliche Bewegung
- âœ… Minimale Energie (kleine Penalties)
- âœ… Smooth Actions (action_rate penalty bleibt)

**Rewards fÃ¶rdern:**
- Stehen bleiben (alive + upright)
- FÃ¼ÃŸe am Boden (foot_contact)
- Kein Umfallen (termination penalty)

**Rewards bestrafen:**
- Zu viel Bewegung (base_vel penalty reduziert)
- Zu schnelle Joints (joint_vel penalty reduziert)
- Hohe Torques (action penalty bleibt)
- Jerky Actions (action_rate penalty bleibt)

---

## Isaac Lab Referenzen

**Anymal PPO Config (Quelle):**
- `units: [512, 256, 128]`
- `horizon_length: 24`
- `minibatch_size: 16384`
- `learning_rate: 3e-4`
- `e_clip: 0.2`
- `mini_epochs: 5`

**GitHub Links:**
- Humanoid: `https://github.com/isaac-sim/IsaacLab/tree/main/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/classic/humanoid`
- Quadruped: `https://github.com/isaac-sim/IsaacLab/tree/main/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/classic/quadruped`

